---
title: 'Peem Lerdputtipongporn: ADA Week 2'
date: "3rd Feb"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r cars, echo= FALSE}
# Uncomment 
df <- read.csv("/Users/lerdp/Desktop/ADA/rema_dataset_clean.csv")
```

Variables I have:

(Metadata)
[1] "X"                                  "id"                                
 [3] "publishedAt"                        "viewCount"                         
 [5] "likeCount"                          "dislikeCount"                      
 [7] "dislikeCount.1"                     "comment"                           
 [9] "title"                              "description"                       
[11] "contentDuration"                    "keyword"                           
[13] "rank"                               "channelTitle"                      
[15] "channelSubscriberCount"             "channelDescription"                
[17] "channelViewCount"                   "channelVideoCount"                 
[19] "contentDimension"                   "contentDefinition"   
[21] "contentCaption"                     

(Content view)
"totalwords_description"            
[23] "Flesh_kincaid_Grade"                "Automated_Readability_Index"       
[25] "neg"                                "neu"                               
[27] "pos"                                "summary_word_Count"                
[29] "transition_word_Count"              "verb_count"                        
[31] "active_verbs_count"                 "passive_verb_count"                
[33] "description_sentence_count"         "unique_words_description"          
++ Misinformation (TODO: operationalize using metadata).


Typology of authoritative sources? 
- Need some lists of authoritative and find ways to label videos.
- Slant classifier >> need to operationalize slant and apply it to metadata. 
- Idea: Create dictionary 

Medical information 
PEMAT guideline on understandability. 
Actionability. 
Misinformation (<> accuracy). So far, Twitter covid misinformation is studied, but little literature on health-relaed chronic diseases.

Scalability. Map criteria > feature > build classifiers on multiple videos. 


(On understandability)
The Liu et. al. paper used co-training on two sets of covariates: metadata view (video title, description, and tags), and content view (transcript — active word, summary word, transition word, shot count, uique object).
Below are variables in the dataset in each category

# 1. Unused metadata.
** These features are not used in the Ask Your Doctor paper.
[19] "contentDimension"                   "contentDefinition"                  "contentCaption"
"dislikeCount.1"

contentDimension probaby cannot be used because only 1 of 11,000 videos is in 3d — the rest 2D. 60\% of videos are hd — the rest sd. Around 10\% of the videos contain contentCaption. On average, videos with contentCaption and highDefinition enjoy higher viewCounts and engagements. 
                          
#2. How each covariate is used in the model. 

## Key takeaways.

In terms of understandability, the paper asks physicians to rate 600 videos in terms of whether they follow the PEMAT criteria for understandability. Afterwards, it uses co-training (a semi-supervised learning algorithm) to classify videos according to two views — metadata and content. **The dataset provided by Rema contains variables in Section 3.5.2, but not in Section in 3.5.3.**

In terms of content, the paper uses video transcript and description, a method proposed by Liu et. al. (2020). No other explanation provided.

## Section 3.5.2: Video understandability (metadata view)
[10] "description"                        "keyword"**
[28] "summary_word_Count"                 "transition_word_Count"              "verb_count"                        
[31] "active_verbs_count"                 "passive_verb_count"                 "description_sentence_count"        
[34] "unique_words_description"           "contentDuration"
"title"  (Boolean)


## Section 3.5.3: Video understandability (video content view). 

The paper uses three categories of data to measure video content: video narratives, video shots and associated confidence scores. Narratives appears on transcript; video shot from Google intelligence; confidence score from video transcript confidence. 

## Section 4.1: Understandability classification.

First, use co-training on 3.5.2 and 3.5.3 covariates. Experts mark 600 of the videos using PEMAT guidelines, and videos that receive score more than 0.5 are marked understandable. 
** We do not have expert marking in this dataset.

Second, compare co-training models to baseline models (Logistic reg, SVM, random forest) on traditional ML metrics (precision, recall, F-1 harmonic mean). Co-training performs better than baseline.

## Section 4.2. Whether understandable videos make patients more likely to recommend them?

Yes, using precision-AT-k. That is, doctors rate among top K videos (by Youtube), how many will they recommend? Afterwards, compare to top K (in terms of understandability). OUr understandability more likely to be recommended than what Youtube produces. 
[13] "rank"  


## Section 4.3: How does video understandability impact collective Engagement? 
1. First, define collective engagements. The authors use video metadata as proxy for engagement as listed below (for details, see Table A5). Given these variables are correlated, they use PCA on video metadata on 9873 videos to identify three types of engagement: non-engagement, sustained attention, selective attention (interpretation of authors).
[4] "viewCount"                          "likeCount"                          "dislikeCount"                      
[7] "dislikeCount.1"                     "comment"    (number of comments + positive + negative)

2. Second, they categorize videos into 4 types: (high/low understandable/medical info). In their words, "in our study, we have videos classified with both high and low understandability and high
and low medical information. We build upon Liu et al. (2020) to build a measure of high and low
medical information encoded in a video." 

3. Now that they have measures of understandability and three types of engagement from PCA, they use propensity score matching to explore **how does the understandability of a video impact collective engagement?**. They asserted this method is suitable because A) the data is observational: you cannot randomize high/low understandability videos to measure engagement B) you cannot control for confounders.

By definition, propensity score is $P(Y|X)$, so use propensity score matching to construct a counterfactual of individuals not receiving treatment. 4 treatment conditions (i.e. types of videos) are then matched.


# Results 
1. Discussed at Page 28: high understandability negatively impacts disengagement. 


# Observation on the dataset
1. The sentiment is mostly within the 0.95-1.05 range.

# Questions to expert scientists

## About the paper 

\textbf{TO ASK:} In Section 3.5.3, there's "Table 9: Features for Video Understandability Classification from Video Content View." The paper uses video content (narratives, transcripts, shots) to classify understandability. Is this content the same data source/method as medical information as defined in Section 4.2? How do they conceptualize medical information in a video? Is the method/feature similar to video content view? 

\textbf{TO ASK:} How does this medical information in Section 4.3 differ from what is used in the co-training video content view (Section 3.5.3)? I looked into Liu et. al (2020). The transcript and the video description are fed into BLSTM to extract medical words. Ask the researchers to explain what they are trying to achieve and how-why they use BLSTM.

\textbf{TO ASK:} Why bother characterize low-high medical information (leading to 4 treatments) when we already have low-high understandability (2 treatments)? 


\textbf{TO CLARIFY:} What propensity are we matching, and for what purpose? Although the paper mentions their propensity score matching controls for content providers (i.e. channels), I'm not sure if/how they use these variables. 

"channelTitle"                       "channelSubscriberCount"            
"channelDescription"                 "channelViewCount"                   "channelVideoCount"

\textbf{TO REQUEST:} Can the researchers provide 
1. video content labels for video understandability as well as $Y$
2. source code to replicate results in the paper
3. reformatted dataset (refer to exchange with Prof. Liu)?


## About the dataset
1. Q: The dataset I have doesn't contain labels on understandability or medical content. Would it be possible for me to obtain them? As of now, we have covariates (X), but we don't have labels (Y) on understandability in this dataset, be it from the original 600 or results of semi-supervised learning. 
2. Q: According to wiki, readability index should be around 1-15. The third quantile is 15, but there are around 1000 videos with more than 20 (with some having 392).
3. Q: What is keyword? Is it the same thing as tag in your paper?
4. Q: What is ranking? Is it the ranking of video understandability or Youtube search? 
5. How is understandability ranked? There's a PEMAT formula for understandability, but if that score exceeds 0.5, then the label is understandable (1). If so, how can we rank understandability if their values are binary? 


## Notes to Nynke and Larry:
1. Some explanation on causal inference confuses me. Here's their quote on multiple propensity score matching (Page 27-28). Can you help me understand the statistics of these parts?  
"characteristics of videos are not exogenous, rather, they are determined by video level features,
content creator features and the reputation of the content provider etc., which makes the
dimensions of medical information and understandability endogenous, lending itself to a treatment
effects type of causal estimation" (Page 26-27).

2. Last time we met, we were confused about what is medical information in this dataset. My answer: it's a combination of video description (which we currently have) and video transcript (which we don't). Another paper written by them — Liu et. al. 2020 — uses BLSTM on transcripts to do so. 

3. Hammer down co-training, BLSTM for medical content, and ML algorithms. 

```{r pressure, echo=FALSE}
par(mfrow = c(1,3))
hist(df$pos, freq = TRUE)
hist(df$neg, freq = TRUE)
hist(df$neu, freq = TRUE)
df$sentiment = df$pos + df$neg + df$neu

# NOTEWORTHY OBSERVATION


temp <- subset(df, sentiment > 1.05 || sentiment < 0.95)


```

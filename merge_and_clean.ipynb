{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f8ee330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is for Peem's attempt to fit all models using labelled data.\n",
    "# Last updated: 18th Feb 2022. \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import altair as alt\n",
    "pd.options.display.float_format = '{:20,.2f}'.format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd259357",
   "metadata": {},
   "source": [
    "# Executive Summary\n",
    "\n",
    "This Jupyter notebook is separated into two parts:\n",
    "1. Merging and cleaning multiple datasets. Each of the PEMAT labels are used to calculated labels on understandability, actionability, and medical information (already exists). \n",
    "2. Created a dictionary of PEMAT Question (Key) and its definition (Value).  \n",
    "3. Performing EDA on 600 videos with PEMAT labels (see description in Part 2).\n",
    "\n",
    "NOTE: I didn't remove entry 275 (URL = 'RQCq-FzeYgA') even though its PEMAT label is entirely NaN. Make sure to remove it in the final analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b171aa7",
   "metadata": {},
   "source": [
    "# Part 1: Cleaning and merging the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b61c23b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"content.csv\", sep = \",\")\n",
    "# Downloading the labelled dataset.\n",
    "df_label = pd.read_csv(\"label600.csv\", sep = \",\", encoding=\"ISO-8859-1\")\n",
    "df_label = df_label.T.set_index(0).T\n",
    "df_label = df_label.dropna(axis = 1, how = \"all\")\n",
    "# Downloading the separate metadata and engagement. \n",
    "df_meta = pd.read_csv(\"metadata.csv\", sep = \",\")\n",
    "df_engagement = pd.read_csv(\"engagement.csv\", sep = \",\")\n",
    "\n",
    "# Merge content, metadata, and engagement.\n",
    "# Use the set operation to avoid column duplicates. \n",
    "first = set(df.columns)\n",
    "second = set(df_meta.columns)\n",
    "third = set(df_engagement.columns)\n",
    "second = second - first\n",
    "third = (third - second) - first\n",
    "final = list(first) + list(second) + list(third)\n",
    "\n",
    "# Concatenating all of the dataframe\n",
    "dftemp = pd.concat([df, df_meta[second], df_engagement[third]], axis = 1)\n",
    "df = dftemp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a348f16a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    ['/m/01k8wb', '/m/098wr', '/m/098wr']\n",
       "1                                 ['/m/098wr', '/m/098wr']\n",
       "2                                 ['/m/098wr', '/m/098wr']\n",
       "3        ['/m/019_rr', '/m/0kt51', '/m/019_rr', '/m/0kt...\n",
       "4                               ['/m/01k8wb', '/m/01k8wb']\n",
       "                               ...                        \n",
       "11138    ['/m/02wbm', '/m/019_rr', '/m/019_rr', '/m/02w...\n",
       "11139    ['/m/02wbm', '/m/019_rr', '/m/098wr', '/m/098wr']\n",
       "11140    ['/m/02wbm', '/m/019_rr', '/m/019_rr', '/m/02w...\n",
       "11141                ['/m/019_rr', '/m/098wr', '/m/098wr']\n",
       "11142                                         ['/m/098wr']\n",
       "Name: relevantTopicIds, Length: 11143, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Find how many topics there are, do the connection. \n",
    "# DO the weighting. Linked by at least one video.\n",
    "df[\"relevantTopicIds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23688156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to clean the URL in the dataset. \n",
    "def get_id(full_link):\n",
    "    temp = full_link.split(\"=\")\n",
    "    return temp[1]\n",
    "\n",
    "# A function to obtain mapping between PEMAT criterion and its value.\n",
    "def PEMAT_map(df):\n",
    "    \"\"\"\n",
    "    This function accepts the dataframe and returns the dictionary that maps each PEMAT criteria \n",
    "    number to its description.\n",
    "    \"\"\"\n",
    "    PEMAT_dict = {}\n",
    "    for col_name in df.columns.tolist():\n",
    "        try:\n",
    "            split = col_name.split(\".\")\n",
    "            key = split[0]\n",
    "            temp = \"\"\n",
    "            if len(split) > 1:\n",
    "                # Concatenate every remaining string\n",
    "                for i in range(1, len(split)):\n",
    "                    temp += str(split[i])\n",
    "            value = temp\n",
    "            if key != 'This video contain high medical knowledge (0: low; 1: High)':\n",
    "                PEMAT_dict[key] = value         \n",
    "        except:\n",
    "            pass\n",
    "    PEMAT_dict[\"info\"] = \"This video contain high medical knowledge (0: low; 1: High)':\"\n",
    "    return PEMAT_dict\n",
    "\n",
    "# Test if the dictionary is obtained.\n",
    "PEMAT_dict = PEMAT_map(df_label)\n",
    "\n",
    "def greater05(value):\n",
    "    if value >= 0.5:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6285974b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the dictionary\n",
    "import pickle \n",
    "d_file = open(\"PEMAT_dict.pkl\", \"wb\")\n",
    "pickle.dump(PEMAT_dict, d_file)\n",
    "d_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "418b7245",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedWriter name='PEMAT_dict.pkl'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "567c5f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_file = open(\"PEMAT_dict.pkl\", \"rb\")\n",
    "output = pickle.load(d_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "999cb3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the column\n",
    "temp = {}\n",
    "original_col = df_label.columns.tolist()\n",
    "assert len(original_col) == len(list(PEMAT_dict.keys()))\n",
    "for i, replacer in enumerate(list(PEMAT_dict.keys())):\n",
    "    temp[original_col[i]] = replacer\n",
    "df_label = df_label.rename(columns = temp)\n",
    "\n",
    "# Obtaining the URL\n",
    "df_label[\"URL\"] = df_label.apply(lambda row: get_id(row[\"URL\"]), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a7cd85",
   "metadata": {},
   "source": [
    "# Part 1.1: Checking the distribution of PEMAT labels/medical information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6dd4773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we have two dataframes: df_label (containing 600 labels), and df (containing 11000 labels).\n",
    "# We want to calculate actionability and understandability.\n",
    "actionable = [str(i) for i in [20,21,22,25]]\n",
    "understandable = [str(i) for i in [1,3,4,5,8,9,10,11,13,14,18,19]]\n",
    "\n",
    "# Change the type from string to int. \n",
    "df_label[actionable] = df_label[actionable].apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "df_label[understandable] = df_label[understandable].apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "\n",
    "# Calculate the mean\n",
    "df_label[\"action\"] = df_label[actionable].mean(axis = 1, skipna = True, numeric_only = True)\n",
    "df_label[\"understand\"] = df_label[understandable].mean(axis = 1, skipna = True, numeric_only = True)\n",
    "\n",
    "# Apply indicator function whether the mean is greater than 0.5\n",
    "df_label[\"action\"] = df_label.apply(lambda row: greater05(row[\"action\"]), axis = 1)\n",
    "df_label[\"understand\"] = df_label.apply(lambda row: greater05(row[\"understand\"]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a45b544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are the list of variables I dropped\n",
      "has_tags\n",
      "has_title\n",
      "audioTrackType\n",
      "privacyStatus\n",
      "favoriteCount\n",
      "isCC\n",
      "contentRating.ytRating\n",
      "uploadStatus\n",
      "language\n"
     ]
    }
   ],
   "source": [
    "# Checking if there's any column with only one value (i.e. if every element is the same, cannot be used.)\n",
    "to_drop = []\n",
    "for i, col in enumerate(df.columns):\n",
    "    if df[col].value_counts().shape[0] == 1: # Only one type, so cannot be used for prediction. \n",
    "        to_drop.append(col)\n",
    "df = df.drop(columns = to_drop)\n",
    "print(\"Below are the list of variables I dropped\")\n",
    "for col in to_drop:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e9e1fab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After removing all columns with one input, I saved the 12k without labels into a dataframe.\n",
    "df.to_csv(\"merged_and_cleaned12k.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a6b4e2",
   "metadata": {},
   "source": [
    "# Part 1.3: Merging videos with PEMAT labels with overall 12k.\n",
    "In this part, I merge df (12k) with df_label (600) based on URL. Afterwards, we will obtain a dataset with 600 labelled videos along with all of its information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd783f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the matching based on 'URL' in df_label and 'id' in df.\n",
    "\n",
    "tempdf = df_label[[\"URL\", \"Duration\",\"info\", \"action\", \"understand\"]]\n",
    "\n",
    "# Find all videos in the large dataset with labels. \n",
    "# Results: The dataset has some duplicate. \n",
    "tempdf = tempdf.set_index(\"URL\")\n",
    "try:\n",
    "    df = df.set_index(\"video_id\")\n",
    "except:\n",
    "    pass\n",
    "newdf = tempdf.join(df)\n",
    "\n",
    "# Dropping duplicate entries and storing the cleaned dataset\n",
    "try:\n",
    "    newdf.reset_index(inplace = True)\n",
    "except:\n",
    "    pass \n",
    "newdf = newdf.rename(columns = {\"index\":\"URL\"})\n",
    "newdf = newdf.drop_duplicates(\"URL\")\n",
    "df = newdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eb30ad04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.00    421\n",
       "0.00    199\n",
       "Name: info, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert every features whose values can be interpreted as numbers.\n",
    "for col in newdf.columns.tolist():\n",
    "    newdf[col] = pd.to_numeric(newdf[col], errors = \"ignore\")\n",
    "newdf[\"info\"].value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54f07395",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"merged_and_cleaned600.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9b5ed260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    14.52\n",
       "2                     9.06\n",
       "4                    21.83\n",
       "5                    15.78\n",
       "6                    12.51\n",
       "              ...         \n",
       "882                  19.30\n",
       "883                   8.49\n",
       "884                  16.95\n",
       "885                   9.27\n",
       "886                  20.22\n",
       "Name: ARI, Length: 621, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"ARI\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250bfc50",
   "metadata": {},
   "source": [
    "# Part 2: Checking PEMAT input per questions\n",
    "\n",
    "In the above part, I have merged Xiao's datasets and created two versions: merged_and_cleaned with labels (600) and merged_and_cleaned without labels (11000). The remaining task would be to join them by row_id, if need be. However, it's interesting to examine df_label itself to see what the distribution of each questions are.\n",
    "\n",
    "## Missing values\n",
    "For most of the columns, there are only few missing entries. The noteworthy columns with high misses are 13, 18, 19, 25, which corresponds to clarity of simple graphs/illustrations/etc. These questions may not be applicable to all diabetes videos. Therefore, there's nothing egregious about missingness. \n",
    "\n",
    "## Strange values\n",
    "Medical information (i.e. 'info') has a lot of non-sense values. \n",
    "\n",
    "## Observations\n",
    "1. 75% of the videos are understandable (1-19), 25% are not. A simple rule of outputting 1 (i.e. every video is understandable) will achieve 75% accuracy, so **need to think about alternative metrics**. 47% of videos are actionable (corresponding to Question 20,21,22,25).\n",
    "\n",
    "2. Because some PEMAT questions have NaN, I cannot calculate the correlation between each of the response questions.\n",
    "TODO: Ask Larry/Nynke if there's a need to calculate the correlation.\n",
    "\n",
    "3. Actionability and understandability have the correlation coefficient of 0.128. This value is very low. \n",
    "\n",
    "4. Duration (unit: second) is skewed heavily to the right â€” there are some very lengthy videos. **Need to log-scale if included in the final model.**\n",
    "\n",
    "5. PEMAT criteria with mostly zero entries are 8,9,19,22 (check PEMAT_dict). They all correspond to not breaking down information into small-chunks/actionable steps or lack of informative headers. **This could inform why videos we classify as zero are not understandable/actionable**. \n",
    "\n",
    "6. \"11\" (summary) and \"25\" (graphs/charts to take actions) do not receive good scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "931e88e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0\n",
       "URL             0\n",
       "Title           0\n",
       "Duration        0\n",
       "1               1\n",
       "3               1\n",
       "4               1\n",
       "5               1\n",
       "8               4\n",
       "9               5\n",
       "10              1\n",
       "11              3\n",
       "13            231\n",
       "14             38\n",
       "18            200\n",
       "19            387\n",
       "20              2\n",
       "21              2\n",
       "22              2\n",
       "25            342\n",
       "info            1\n",
       "action          0\n",
       "understand      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking missing values of PEMAT response in each column. \n",
    "df_label.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "341dce33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Duration</th>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>25</th>\n",
       "      <th>info</th>\n",
       "      <th>action</th>\n",
       "      <th>understand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>60.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6,502.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median</th>\n",
       "      <td>317.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>522.88</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0.89</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.52</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "0                  Duration                    1                    3  \\\n",
       "min                   60.00                 0.00                 0.00   \n",
       "max                6,502.00                 1.00                 1.00   \n",
       "median               317.00                 1.00                 1.00   \n",
       "mean                 522.88                 0.83                 0.78   \n",
       "\n",
       "0                         4                    5                    8  \\\n",
       "min                    0.00                 0.00                 0.00   \n",
       "max                    1.00                 1.00                 1.00   \n",
       "median                 1.00                 1.00                 0.00   \n",
       "mean                   0.63                 0.80                 0.18   \n",
       "\n",
       "0                         9                   10                   11  \\\n",
       "min                    0.00                 0.00                 0.00   \n",
       "max                    1.00                 1.00                 1.00   \n",
       "median                 0.00                 1.00                 0.00   \n",
       "mean                   0.16                 0.80                 0.32   \n",
       "\n",
       "0                        13                   14                   18  \\\n",
       "min                    0.00                 0.00                 0.00   \n",
       "max                    1.00                 1.00                 1.00   \n",
       "median                 1.00                 1.00                 1.00   \n",
       "mean                   0.77                 0.89                 0.80   \n",
       "\n",
       "0                        19                   20                   21  \\\n",
       "min                    0.00                 0.00                 0.00   \n",
       "max                    1.00                 1.00                 1.00   \n",
       "median                 0.00                 0.00                 1.00   \n",
       "mean                   0.22                 0.49                 0.52   \n",
       "\n",
       "0                        22                   25                 info  \\\n",
       "min                    0.00                 0.00                  NaN   \n",
       "max                    1.00                 1.00                  NaN   \n",
       "median                 0.00                 0.00                 1.00   \n",
       "mean                   0.18                 0.41                  NaN   \n",
       "\n",
       "0                    action           understand  \n",
       "min                    0.00                 0.00  \n",
       "max                    1.00                 1.00  \n",
       "median                 0.00                 1.00  \n",
       "mean                   0.47                 0.75  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distribution\n",
    "df_label[\"Duration\"] = pd.to_numeric(df_label[\"Duration\"],)\n",
    "numerical_col = df_label.columns.tolist()\n",
    "numerical_col.remove(\"Title\")\n",
    "numerical_col.remove(\"URL\")\n",
    "fare = [\"min\", \"max\", \"median\", \"mean\"]\n",
    "temp = {}\n",
    "for col in numerical_col:\n",
    "    temp[col] = fare\n",
    "df_label.agg(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2819d349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.12826087],\n",
       "       [0.12826087, 1.        ]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A code to find the covaraince matrix for each of the PEMAT questions. \n",
    "temp = []\n",
    "for col in  df_label.columns.tolist()[3:19]:\n",
    "    temp.append(df_label[col].tolist())\n",
    "temp = np.array(temp)\n",
    "# TODO: uncomment. np.corrcoef(temp)\n",
    "\n",
    "# Find the correlation between understand and action\n",
    "act_under = np.array([df_label[\"action\"].tolist(), df_label[\"understand\"]])\n",
    "np.corrcoef(act_under)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "633cb3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the location where info label is unavailable.\n",
    "np.where(df_label[\"info\"].isna() == True)\n",
    "df = df.drop(index = [275])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cda682af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the dataframe\n",
    "try:\n",
    "    readability_df = readability_df.set_index(\"video_id\")\n",
    "    df = df.set_index(\"URL\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Re-assign readability indices\n",
    "for index in df.index.tolist():\n",
    "    if index in readability_df.index.tolist():\n",
    "        df.loc[index, \"ARI\"] = readability_df.loc[index, \"ari\"] \n",
    "        df.loc[index, \"FleshReadingEase\"] = readability_df.loc[index, \"flesch\"]\n",
    "        df.loc[index, \"Kincaid\"] = readability_df.loc[index, \"kincaid\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bfe02eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all files\n",
    "df_label.to_csv(\"rawPEMAT.csv\")\n",
    "df.to_csv(\"merged_and_cleaned600.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

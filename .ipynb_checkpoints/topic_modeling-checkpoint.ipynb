{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "846ed365",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_fn import *\n",
    "import re\n",
    "import spacy\n",
    "import scispacy\n",
    "from spacy import displacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "# For LDA \n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import LdaMulticore\n",
    "import gensim.corpora as corpora\n",
    "# For word clouds\n",
    "from wordcloud import WordCloud\n",
    "# For most frequent words\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46896082",
   "metadata": {},
   "source": [
    "# TODO LIST (April 17st, 2022) \n",
    "\n",
    "1. Most common word counts using spacy. Biomedical terms recognition.\n",
    "2. LDA on larger bag of words. To ask: How did you decide on those 600 videos? \n",
    "3. Identify words that have highest weight for each video. Pick most common topics between the two.\n",
    "Top 10 most common words of understandable videos contain more diabetes-related terms than those that don't. However,\n",
    "I haven't normalized the word per duration yet due to storing issue. E.g., genesthey&#39;re\n",
    "4. Descriptive statisitcs for each class (Done)\n",
    "5. LASSO sparse regression on word counts and other variables Xiao used.\n",
    "\n",
    "6. Can embed the similarity between words\n",
    "7. Sentiment analysis\n",
    "\n",
    "Summary on misinformation\n",
    "- 469 (3/4) of the videos are labelled, of which 30% are not misinformation and the rest are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "0663a05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download videos with subtitles and dictionary of videos (key = ID, labels = subtitle)\n",
    "df600 = pd.read_csv(\"merged_and_cleaned600.csv\", sep = \",\").drop(columns = [\"Unnamed: 0\"])\n",
    "outdir = \"/Users/lerdp/Desktop\"\n",
    "l_file = open(\"%s/labelled_subtitle.pkl\" %(outdir),'rb')\n",
    "ul_file = open(\"%s/unlabelled_subtitle.pkl\" %(outdir), 'rb')\n",
    "l_subtitle = pickle.load(l_file)\n",
    "ul_subtitle = pickle.load(ul_file)\n",
    "l_file.close()\n",
    "ul_file.close()\n",
    "\n",
    "# Define nlp spacy\n",
    "nlp_medical = spacy.load(\"en_core_sci_md\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "b0dfd8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getID(URL):\n",
    "    if URL == None:\n",
    "        return\n",
    "    else:\n",
    "        URL = re.sub(r'http\\S+=', '', URL)\n",
    "        URL = re.sub(r'http\\S+/', '', URL)\n",
    "        return URL\n",
    "\n",
    "def isMisinfo(text):\n",
    "    if text.lower().strip() == \"yes\":\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def getWordBag(doc):\n",
    "    allwords = [token.lemma_ for token in doc if not token.is_punct and not token.is_stop \n",
    "                                              and not token.is_space and token.is_alpha]\n",
    "    medwords = [ent.lemma_ for ent in doc.ents]\n",
    "    return allwords, medwords\n",
    "\n",
    "def wordFrequency(word_lst, ntop = 5, normalize = False):\n",
    "    temp = {}\n",
    "    total_word = 0\n",
    "    for word in word_lst:\n",
    "        if word.strip() != '': # Not an empty string\n",
    "            try:\n",
    "                temp[word] += 1\n",
    "            except:\n",
    "                temp[word] = 1\n",
    "            total_word += 1\n",
    "    if normalize:\n",
    "        for word in list(temp.keys()):\n",
    "            temp[word] = round(temp[word]/total_word, 4)\n",
    "    return dict(Counter(temp).most_common(ntop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3fb34240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download misinformation labels and rename columns\n",
    "df_mis = pd.read_csv(\"misinformation.csv\", lineterminator = \"\\r\")\n",
    "df_mis = df_mis.drop(columns = df_mis.columns[4:]).dropna(subset = [\"URL\"])\n",
    "df_mis[\"video_id\"] = df_mis[\"URL\"].map(getID)\n",
    "\n",
    "# Rename the columns\n",
    "rename = {}\n",
    "for col in df_mis.columns.tolist():\n",
    "    rename[col] = col.lower().strip()\n",
    "df_mis = df_mis.rename(columns=rename)\n",
    "\n",
    "# Obtain the video_id\n",
    "cur_index = df_mis[df_mis[\"title\"] == \"Latent autoimmune diabetes of adults\"].index.tolist()[0]\n",
    "df_mis = df_mis[:cur]\n",
    "\n",
    "# Convert misinformation into indicator\n",
    "df_mis[\"misinformation\"] = df_mis[\"misinformation\"].fillna(\"No\")\n",
    "df_mis[\"misinformation\"] = df_mis[\"misinformation\"].map(isMisinfo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "a4ce6302",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here\n",
      "Hello\n",
      "Hello\n"
     ]
    }
   ],
   "source": [
    "all_id = list(l_subtitle.keys()) + list(ul_subtitle.keys())\n",
    "mis_nnosub = 0\n",
    "cor_nnosub = 0\n",
    "mis_text = \"\"\n",
    "cor_text = \"\"\n",
    "\n",
    "mis_mostfreqwords = pd.DataFrame(index = )\n",
    "mis_mostfreqwords = pd.DataFrame\n",
    "# Obtain text for correct videos\n",
    "for video_id in df_mis[df_mis[\"misinformation\"] == 0][\"video_id\"]:\n",
    "    if video_id in all_id:\n",
    "        try:\n",
    "            subtitle += l_subtitle[video_id]\n",
    "        except:\n",
    "            subtitle += ul_subtitle[video_id]\n",
    "        cor_text += subtitle\n",
    "\n",
    "    else:\n",
    "        cor_nnosub += 1\n",
    "        \n",
    "# Obtain all texts for misinformation videos\n",
    "for video_id in df_mis[df_mis[\"misinformation\"] == 1][\"video_id\"]:\n",
    "    if video_id in all_id:\n",
    "        try:\n",
    "            mis_text += l_subtitle[video_id]\n",
    "        except:\n",
    "            mis_text += ul_subtitle[video_id]\n",
    "        \n",
    "    else:\n",
    "        mis_nnosub += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8b2b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "mis_allfreqlst = []\n",
    "mis_medfreqlst = []\n",
    "mis_sublst = []\n",
    "cor_allfreqlst = []\n",
    "cor_medfreqlst = []\n",
    "cor_sublst = []\n",
    "emptysub = []\n",
    "ntop = 15\n",
    "\n",
    "# Correct videos: Create a list of subtitles for each video; two dictionaries for all and medical frequency\n",
    "for video_id in df_mis[df_mis[\"misinformation\"] == 0][\"video_id\"]:\n",
    "    if video_id in all_id:\n",
    "        try:\n",
    "            subtitle = l_subtitle[video_id]\n",
    "        except:\n",
    "            subtitle = ul_subtitle[video_id]\n",
    "      \n",
    "        if subtitle.strip() != \"\":\n",
    "            # Modify the line below for processing\n",
    "            doc_medical = nlp_medical(subtitle)\n",
    "            allwords, medwords = getWordBag(doc_medical)\n",
    "            cor_allfreqlst.append(wordFrequency(allwords, ntop = ntop, normalize = True))\n",
    "            cor_medfreqlst.append(wordFrequency(medwords, ntop = ntop, normalize = True))\n",
    "        else: \n",
    "            emptysub.append(video_id)\n",
    "            \n",
    "# Misinformation videos: Create a list of subtitles for each video; two dictionaries for all and medical frequency\n",
    "for video_id in df_mis[df_mis[\"misinformation\"] == 1][\"video_id\"]:\n",
    "    if video_id in all_id:\n",
    "        try:\n",
    "            subtitle = l_subtitle[video_id]\n",
    "        except:\n",
    "            subtitle = ul_subtitle[video_id]\n",
    "        if subtitle.strip() != \"\":\n",
    "            mis_sublst.append(subtitle)\n",
    "            # Modify the line below for processing\n",
    "            doc_medical = nlp_medical(subtitle)\n",
    "            allwords, medwords = getWordBag(doc_medical)\n",
    "            mis_allfreqlst.append(wordFrequency(allwords, ntop = ntop, normalize = True))\n",
    "            mis_medfreqlst.append(wordFrequency(medwords, ntop = ntop, normalize = True))\n",
    "        else:  # Exclude all videos with empty subtitles\n",
    "            emptysub.append(video_id)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "0b045505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mis_medfreqlst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "fdeb36dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_medical_misinfo = nlp_medical(mis_text)\n",
    "doc_medical_cor = nlp_medical(cor_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "06a820e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29042"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_medical_misinfo.ents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "5f6cdca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'blood': 0.0202,\n",
       " 'diabete': 0.0197,\n",
       " 'insulin': 0.0181,\n",
       " 'sugar': 0.0133,\n",
       " 'type': 0.0113,\n",
       " 'glucose': 0.011,\n",
       " 'cell': 0.0089,\n",
       " 'level': 0.0077,\n",
       " 'know': 0.0071,\n",
       " 'like': 0.0069}"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allwords_cor, medwords_cor = getWordBag(doc_medical_cor)\n",
    "wordFrequency(allwords_cor, ntop = 10, normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "9f33d37f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'blood': 0.0116,\n",
       " 'diabete': 0.011,\n",
       " 'go': 0.0085,\n",
       " 'like': 0.0075,\n",
       " 'insulin': 0.0074,\n",
       " 'cell': 0.0073,\n",
       " 'sugar': 0.0066,\n",
       " 'type': 0.0065,\n",
       " 'know': 0.0058,\n",
       " 'body': 0.0058}"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allwords_misinfo, medwords_misinfo = getWordBag(doc_medical_misinfo)\n",
    "wordFrequency(allwords_misinfo, ntop = 10, normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "08355fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#displacy.render(next(doc.sents), style='dep', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34923d5a",
   "metadata": {},
   "source": [
    "# Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20ebe3e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.040*\" \" + 0.011*\"like\" + 0.010*\"go\" + 0.008*\"know\" + 0.008*\"blood\" + 0.008*\"diabetes\" + 0.006*\"insulin\" + 0.006*\"time\" + 0.005*\"sugar\" + 0.005*\"thing\"')\n",
      "(1, '0.035*\" \" + 0.010*\"know\" + 0.009*\"like\" + 0.009*\"blood\" + 0.008*\"insulin\" + 0.008*\"go\" + 0.007*\"sugar\" + 0.006*\"al\" + 0.006*\"diabetes\" + 0.005*\"patient\"')\n",
      "(2, '0.040*\" \" + 0.013*\"insulin\" + 0.013*\"diabetes\" + 0.011*\"go\" + 0.009*\"like\" + 0.009*\"blood\" + 0.008*\"know\" + 0.008*\"sugar\" + 0.006*\"glucose\" + 0.006*\"type\"')\n"
     ]
    }
   ],
   "source": [
    "# Building a bag of subtitles for each video\n",
    "# See https://radimrehurek.com/gensim/corpora/dictionary.html\n",
    "# See https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0\n",
    "# Create a list of words for each video subtitle.\n",
    "\"\"\"\n",
    "temp = df600[df600[\"info\"] == 1][\"subtitle\"]\n",
    "subtitle_lst = [cleanText(subtitle, stopword_lst, return_string = False) for subtitle in temp]\n",
    "\"\"\"\n",
    "\n",
    "subtitle_lst = [processText(subtitle_dict[keys], custom_nlp = nlp, return_string = False) \n",
    "                for keys in list(subtitle_dict.keys())]\n",
    "# Convert each word into dictionary. This allows us to perform mapping in the future.\n",
    "id2word = corpora.Dictionary(subtitle_lst)\n",
    "# Build a corpus in term document frequency\n",
    "corpus = [id2word.doc2bow(subtitle) for subtitle in subtitle_lst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2ec2b153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.036*\" \" + 0.009*\"like\" + 0.009*\"go\" + 0.008*\"diabetes\" + 0.008*\"know\" + 0.007*\"insulin\" + 0.006*\"patient\" + 0.006*\"want\" + 0.005*\"blood\" + 0.005*\"time\"')\n",
      "(1, '0.041*\" \" + 0.010*\"blood\" + 0.009*\"diabetes\" + 0.009*\"go\" + 0.009*\"sugar\" + 0.008*\"know\" + 0.008*\"like\" + 0.007*\"insulin\" + 0.006*\"glucose\" + 0.005*\"al\"')\n",
      "(2, '0.043*\" \" + 0.014*\"like\" + 0.012*\"go\" + 0.011*\"know\" + 0.010*\"diabetes\" + 0.010*\"blood\" + 0.010*\"insulin\" + 0.006*\"type\" + 0.006*\"patient\" + 0.005*\"sugar\"')\n",
      "(3, '0.034*\" \" + 0.013*\"insulin\" + 0.010*\"diabetes\" + 0.009*\"blood\" + 0.008*\"go\" + 0.007*\"sugar\" + 0.007*\"know\" + 0.007*\"like\" + 0.006*\"glucose\" + 0.005*\"time\"')\n"
     ]
    }
   ],
   "source": [
    "# number of topics\n",
    "num_topics = 4\n",
    "# Build LDA model\n",
    "lda_model = LdaMulticore(corpus=corpus,id2word=id2word, num_topics=num_topics)\n",
    "\n",
    "# Print the Keyword in the topics\n",
    "# Possible interpretation: Different aspects of diabetes.\n",
    "# Word counts on two classes\n",
    "for i in range(len(lda_model.print_topics())):\n",
    "    print(lda_model.print_topics()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ea6a59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

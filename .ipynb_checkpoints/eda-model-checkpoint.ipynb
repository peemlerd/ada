{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ddbe773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.float_format = '{:20,.2f}'.format\n",
    "import matplotlib.pyplot as plt\n",
    "import altair as alt\n",
    "import pickle\n",
    "import string\n",
    "import random\n",
    "\n",
    "# Statistics library\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from helper_fn import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53987b4",
   "metadata": {},
   "source": [
    "## Downloading the following dataframes\n",
    "\n",
    "1. df\n",
    "2. df600\n",
    "3. df_label: PEMAT-labelled dataset.\n",
    "\n",
    "NOTE: https:// creates problems with word_count distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29e25845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: df should have 56 columns because we don't have labels on actionability, understandability, etc.\n",
    "df = pd.read_csv(\"merged_and_cleaned12k.csv\", sep = \",\").dropna(how = \"all\").drop(columns = [\"Unnamed: 0\"])\n",
    "df600 = pd.read_csv(\"merged_and_cleaned600.csv\", sep = \",\").drop(columns = [\"Unnamed: 0\"])\n",
    "df_label = pd.read_csv(\"rawPEMAT.csv\", sep = \",\").drop(columns = [\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b028ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dictionary of PEMAT. \n",
    "# Plan: Work back and forth between the PEMAT raw file (df_label) and the cleaned file (df600).\n",
    "f = open(\"PEMAT_dict.pkl\",\"rb\")\n",
    "PEMAT_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bc7aba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge videos that appear multiple times under different keywords. \n",
    "groupby_column = 'video_id'\n",
    "aggregate_column = 'keyword'\n",
    "agg_df = df.groupby(groupby_column).aggregate({aggregate_column: list})\n",
    "df_alias = df.drop(columns=aggregate_column).set_index(groupby_column)\n",
    "agg_df = agg_df.join(df_alias).reset_index(groupby_column).drop_duplicates(groupby_column).reset_index(drop=True)\n",
    "\n",
    "df = agg_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5767ec9d",
   "metadata": {},
   "source": [
    "# PART 1: Perform the visual EDA. \n",
    "\n",
    "First, I examined the features and classified them into three categories. \n",
    "1. Video metadata\n",
    "\n",
    "2. Video \n",
    "\n",
    "3. Usage statistics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589fb67c",
   "metadata": {},
   "source": [
    "# PART 2: Descriptive Statistics\n",
    "\n",
    "**NOTE: See merge_and_cleaned for a description on the PEMAT labels. This is a descriptive statistics on the entire dataset.\n",
    "\n",
    "## Useful resource\n",
    "1. ARI: https://en.wikipedia.org/wiki/Automated_readability_index\n",
    "2. Flesch reading ease:\n",
    "3. Kincaid (formally titled: Flesch-Kincaid readability). https://readable.com/readability/flesch-reading-ease-flesch-kincaid-grade-level/\n",
    "4. Cosine similarity: https://www.machinelearningplus.com/nlp/cosine-similarity/\n",
    "\n",
    "## Appropriate ranges\n",
    "1. ARI: 1-14\n",
    "2. Kincaid: 0-19; \n",
    "3. Flesch: 0-100.\n",
    "4. cosine similarity -1 to 1; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56eeefd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the summary statistics for each of these features. \n",
    "fare = [\"min\", \"max\", \"median\", \"mean\"]\n",
    "temp = {}\n",
    "\n",
    "# Labels\n",
    "label = [\"info\",\"understand\",\"action\"]\n",
    "\n",
    "# Video metadata\n",
    "readable = ['ARI', 'FleshReadingEase','Kincaid']\n",
    "\n",
    "# Derivatives of video metadata\n",
    "description_d = [\"word_unique\", \"transition_words\",\"summary_words\",\"active_verb\", \"word_count\", \"sentence_count\"]\n",
    "keyword = ['keyword_title_cosine', 'keyword_decription_cosine']\n",
    "channel = ['channelSubscriberCount', \"channelViewCount\", \"channelCommentCount\", \"channelVideoCount\"] # TODO: Log\n",
    "duration = [\"video_duration\"] # Is already an integer\n",
    "contentLicensed = [\"contentLicensed\"]\n",
    "# Miscellaneous\n",
    "rank = [\"rank\"]\n",
    "content = [\"contentDefinition\", \"contentLicensed\"]\n",
    "\n",
    "# Usage statistics (outcome metrics)\n",
    "like_dislike_view = [\"likeCount\", \"dislikeCount\", \"viewCount\"] # TODO: Log-transform\n",
    "comment_cosine = ['comment_title_cosine', 'comment_description_cosine', ]\n",
    "comment_d = ['commentCount','postive_comment_count','negative_comment_count','neutral_comment_count', \n",
    "           \"comment_unique_words\", \"comment_total_words\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22a24c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sandbox for df600\n",
    "to_summarize = channel\n",
    "for col in readable:\n",
    "    temp[col] = fare\n",
    "df600_nu = df600[df600[\"understand\"] == 0]\n",
    "df600_u = df600[df600[\"understand\"] == 1]\n",
    "df600_u.agg(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613adb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many channels have large total views. PCA for highly correlated variables?\n",
    "var = \"channelViewCount\"\n",
    "thresh = 10**6\n",
    "col = [\"rank\", \"keyword\", \"id\", \"understand\", \"title\", \"channelTitle\"]\n",
    "df600[df600[var] > thresh][col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1c2fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform histogram of the dataset.\n",
    "var = \"channelVideoCount\"\n",
    "entire = True\n",
    "\n",
    "if entire == True:\n",
    "    x = df[var]\n",
    "    name = \"entire dataset\"\n",
    "else:\n",
    "    x = df600[var]\n",
    "    name = \"labelled dataset\"\n",
    "bins = [2**i for i in range(7)]\n",
    "fig = plt.hist(x, density = False, facecolor='b', alpha=0.75)\n",
    "plt.xlabel('%s' %(var))\n",
    "plt.ylabel('Number of videos')\n",
    "plt.title('%s distribution on %s' %(var,name))\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95c7d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if columns of df600 and df are the same.\n",
    "# Only difference: video_duration, understand, action, URL, which all appear in other forms already.\n",
    "for col in df600.columns.tolist():\n",
    "    if col not in df.columns.tolist():\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6b3a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df600[\"contentLicensed\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a91641c",
   "metadata": {},
   "source": [
    "# PART 2: Building models\n",
    "In this part, I used Kincaid as a response variable. The dataset is the miniature 600 versions whereby we have the labels. The logistic regression model is built upon only numerical values. Overall, the accuracy is egregiously high (80%), suggesting that I might overfit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba903f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the indices with no_info. \n",
    "# Why: I have checked the data. These rows contains many NaN.\n",
    "no_info = np.where(df600[\"info\"].isna() == True)[0].tolist()\n",
    "no_info += np.where(df600[\"viewCount\"].isna() == True)[0].tolist()\n",
    "df600 = df600.drop(index = no_info, columns = [\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c78e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature transform the readability indices\n",
    "# Thought 1: Select only ARI due to highly corrleated\n",
    "\n",
    "# ARI should be from 0-14\n",
    "ub = 14\n",
    "lb = 0\n",
    "truncateColumn(df600, lb, ub, \"ARI\")\n",
    "summaryStatistics(df, [\"has_description\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6d488c",
   "metadata": {},
   "outputs": [],
   "source": [
    "need_to_dummy = [\"contentDefinition\"]\n",
    "df600 = pd.get_dummies(df600, columns = need_to_dummy)\n",
    "df600"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a76c235",
   "metadata": {},
   "source": [
    "# Results of each classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b2c052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Use every numerical variable\n",
    "unsure = [\"has_description\"] + [\"active_verb\"]\n",
    "sure = duration + keyword + [\"ARI\"] \n",
    "\n",
    "X = df600[sure + unsure]\n",
    "y = df600[\"understand\"]\n",
    "\n",
    "# Set random_state = 1 to compare between models. \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
    "X_train, X_calib, y_train, y_calib = train_test_split(X, y, test_size = 0.12, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40975f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing for logistic regression\n",
    "model_name = \"Logistic\"\n",
    "X_test, y_test, model = testClassifier(X, y, model_name)\n",
    "createROC(X_test, y_test, model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02e64b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing SVM\n",
    "model_name = \"SVM\"\n",
    "grid = {'kernel':['rbf'], 'C': np.arange(1,10,2)}\n",
    "X_test, y_test, model = testClassifier(X, y, model_name, parameters = grid)\n",
    "createROC(X_test, y_test, model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdbf5b00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Testing Random Forest\n",
    "n_estimators = np.arange(10, 200, 20)\n",
    "max_depth = np.arange(1,5)\n",
    "grid = {'n_estimators':n_estimators, 'max_depth':max_depth}\n",
    "model_name = \"RandomForest\"\n",
    "X_test, y_test, model = testClassifier(X, y, model_name, parameters = grid)\n",
    "createROC(X_test, y_test, model, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122a450f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = 0\n",
    "for i in df[\"subtitle\"].index.tolist():\n",
    "    if len(df.iloc[i][\"subtitle\"]) > 20:\n",
    "        temp += 1\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04e6c4b",
   "metadata": {},
   "source": [
    "# Performing topic modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186fbd3f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

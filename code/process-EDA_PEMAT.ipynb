{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f8ee330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook is for Peem's attempt to fit all models using labelled data.\n",
    "# Last updated: 18th Feb 2022. \n",
    "\n",
    "from helper_fn import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd259357",
   "metadata": {},
   "source": [
    "# Executive Summary\n",
    "\n",
    "This Jupyter notebook is separated into these parts: \n",
    "\n",
    "1. Merging and cleaning multiple datasets. Each of the PEMAT labels are used to calculated labels on understandability, actionability, and medical information (already exists). \n",
    "2. Created a dictionary of PEMAT Question (Key) and its definition (Value).  \n",
    "3. Performing EDA on 600 videos with PEMAT labels (see description in Part 2).\n",
    "\n",
    "**NOTE:** I didn't remove entry 275 (URL = 'RQCq-FzeYgA') even though its PEMAT label is entirely NaN. Make sure to remove it in the final analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b171aa7",
   "metadata": {},
   "source": [
    "# Part 1: Cleaning and merging the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b61c23b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading the labelled dataset.\n",
    "df = pd.read_csv(\"content.csv\", sep = \",\")\n",
    "df_label = pd.read_csv(\"label600.csv\", sep = \",\", encoding=\"ISO-8859-1\")\n",
    "df_label = df_label.T.set_index(0).T\n",
    "df_label = df_label.dropna(axis = 1, how = \"all\")\n",
    "\n",
    "# Downloading the separate metadata and engagement. \n",
    "df_meta = pd.read_csv(\"metadata.csv\", sep = \",\")\n",
    "df_engagement = pd.read_csv(\"engagement.csv\", sep = \",\")\n",
    "\n",
    "# Merge content, metadata, and engagement. Use the set operation to avoid column duplicates. \n",
    "first = set(df.columns)\n",
    "second = set(df_meta.columns)\n",
    "third = set(df_engagement.columns)\n",
    "second = second - first\n",
    "third = (third - second) - first\n",
    "final = list(first) + list(second) + list(third)\n",
    "\n",
    "# Concatenating all of the dataframe\n",
    "dftemp = pd.concat([df, df_meta[second], df_engagement[third]], axis = 1)\n",
    "df = dftemp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a348f16a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                    ['/m/01k8wb', '/m/098wr', '/m/098wr']\n",
       "1                                 ['/m/098wr', '/m/098wr']\n",
       "2                                 ['/m/098wr', '/m/098wr']\n",
       "3        ['/m/019_rr', '/m/0kt51', '/m/019_rr', '/m/0kt...\n",
       "4                               ['/m/01k8wb', '/m/01k8wb']\n",
       "                               ...                        \n",
       "11138    ['/m/02wbm', '/m/019_rr', '/m/019_rr', '/m/02w...\n",
       "11139    ['/m/02wbm', '/m/019_rr', '/m/098wr', '/m/098wr']\n",
       "11140    ['/m/02wbm', '/m/019_rr', '/m/019_rr', '/m/02w...\n",
       "11141                ['/m/019_rr', '/m/098wr', '/m/098wr']\n",
       "11142                                         ['/m/098wr']\n",
       "Name: relevantTopicIds, Length: 11143, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Find how many topics there are for each video. \n",
    "df[\"relevantTopicIds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "23688156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to clean the URL in the dataset. \n",
    "def get_id(full_link):\n",
    "    temp = full_link.split(\"=\")\n",
    "    return temp[1]\n",
    "\n",
    "# A function to obtain mapping between PEMAT criterion and its value.\n",
    "def PEMAT_map(df):\n",
    "    \"\"\"\n",
    "    This function accepts the dataframe and returns the dictionary that maps each PEMAT criteria \n",
    "    number to its description.\n",
    "    \"\"\"\n",
    "    PEMAT_dict = {}\n",
    "    for col_name in df.columns.tolist():\n",
    "        try:\n",
    "            split = col_name.split(\".\")\n",
    "            key = split[0]\n",
    "            temp = \"\"\n",
    "            if len(split) > 1:\n",
    "                # Concatenate every remaining string\n",
    "                for i in range(1, len(split)):\n",
    "                    temp += str(split[i])\n",
    "            value = temp\n",
    "            if key != 'This video contain high medical knowledge (0: low; 1: High)':\n",
    "                PEMAT_dict[key] = value         \n",
    "        except:\n",
    "            pass\n",
    "    PEMAT_dict[\"info\"] = \"This video contain high medical knowledge (0: low; 1: High)':\"\n",
    "    return PEMAT_dict\n",
    "\n",
    "# Test if the dictionary is obtained.\n",
    "PEMAT_dict = PEMAT_map(df_label)\n",
    "\n",
    "def greater05(value):\n",
    "    if value >= 0.5:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6285974b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PEMAT_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m \n\u001b[1;32m      3\u001b[0m d_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPEMAT_dict.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m pickle\u001b[38;5;241m.\u001b[39mdump(\u001b[43mPEMAT_dict\u001b[49m, d_file)\n\u001b[1;32m      5\u001b[0m d_file\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PEMAT_dict' is not defined"
     ]
    }
   ],
   "source": [
    "# Saving the PEMAT dictionary. This allows easier reference in case we need to use the raw data.\n",
    "import pickle \n",
    "d_file = open(\"PEMAT_dict.pkl\", \"wb\")\n",
    "pickle.dump(PEMAT_dict, d_file)\n",
    "d_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "567c5f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_file = open(\"PEMAT_dict.pkl\", \"rb\")\n",
    "output = pickle.load(d_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "999cb3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming the column\n",
    "temp = {}\n",
    "original_col = df_label.columns.tolist()\n",
    "assert len(original_col) == len(list(PEMAT_dict.keys()))\n",
    "for i, replacer in enumerate(list(PEMAT_dict.keys())):\n",
    "    temp[original_col[i]] = replacer\n",
    "df_label = df_label.rename(columns = temp)\n",
    "\n",
    "# Obtaining the URL\n",
    "df_label[\"URL\"] = df_label.apply(lambda row: get_id(row[\"URL\"]), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a7cd85",
   "metadata": {},
   "source": [
    "# Part 1.1: Checking the distribution of PEMAT labels/medical information\n",
    "\n",
    "At this point, we have three objects — df_label (a PEMAT-labelled dataframe of 600 videos), df (a dataframe of 11,000 videos), and the PEMAT dictionary. I want to compute the labels and explore how many are classified as understandable, containing medical information, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6dd4773",
   "metadata": {},
   "outputs": [],
   "source": [
    "actionable = [str(i) for i in [20,21,22,25]]\n",
    "understandable = [str(i) for i in [1,3,4,5,8,9,10,11,13,14,18,19]]\n",
    "\n",
    "# Change the type from string to int. \n",
    "df_label[actionable] = df_label[actionable].apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "df_label[understandable] = df_label[understandable].apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "\n",
    "# Calculate the mean\n",
    "df_label[\"action\"] = df_label[actionable].mean(axis = 1, skipna = True, numeric_only = True)\n",
    "df_label[\"understand\"] = df_label[understandable].mean(axis = 1, skipna = True, numeric_only = True)\n",
    "\n",
    "# Apply indicator function whether the mean is greater than 0.5. \n",
    "# NOTE: There is a cleaner way of finding majority than applying lambda function.\n",
    "# However, I chose this method in case we want to compute labels beyond using majority, \n",
    "# such as ignoring certain PEMAT criterion, weighting certain criteria more heavily, etc. \n",
    "df_label[\"action\"] = df_label.apply(lambda row: greater05(row[\"action\"]), axis = 1)\n",
    "df_label[\"understand\"] = df_label.apply(lambda row: greater05(row[\"understand\"]), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9a45b544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are the list of variables I dropped\n",
      "has_tags\n",
      "has_title\n",
      "isCC\n",
      "privacyStatus\n",
      "uploadStatus\n",
      "contentRating.ytRating\n",
      "favoriteCount\n",
      "audioTrackType\n",
      "language\n"
     ]
    }
   ],
   "source": [
    "# Checking if there's any column with only one value (i.e. if every element is the same, cannot be used.)\n",
    "to_drop = []\n",
    "for i, col in enumerate(df.columns):\n",
    "    if df[col].value_counts().shape[0] == 1: # Only one type, so cannot be used for prediction. \n",
    "        to_drop.append(col)\n",
    "df = df.drop(columns = to_drop)\n",
    "print(\"Below are the list of variables I dropped\")\n",
    "for col in to_drop:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9e1fab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After removing all columns with one input, I saved the 12k without labels into a dataframe.\n",
    "# TODO: When Xiao provides the label for 12k, I will re-use it in the dataset.\n",
    "df[\"categoryId\"] = [str(category) for category in df[\"categoryId\"]]\n",
    "df.to_csv(\"merged_and_cleaned12k.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "801bf705",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARI</th>\n",
       "      <th>FleshReadingEase</th>\n",
       "      <th>Kincaid</th>\n",
       "      <th>active_verb</th>\n",
       "      <th>has_description</th>\n",
       "      <th>id</th>\n",
       "      <th>sentence_count</th>\n",
       "      <th>summary_words</th>\n",
       "      <th>transition_words</th>\n",
       "      <th>video_duration</th>\n",
       "      <th>...</th>\n",
       "      <th>channelVideoCount</th>\n",
       "      <th>keyword_decription_cosine</th>\n",
       "      <th>comment_total_words</th>\n",
       "      <th>negative_comment_count</th>\n",
       "      <th>comment_description_cosine</th>\n",
       "      <th>keyword_title_cosine</th>\n",
       "      <th>comment_title_cosine</th>\n",
       "      <th>postive_comment_count</th>\n",
       "      <th>comment_unique_words</th>\n",
       "      <th>neutral_comment_count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>video_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>bJKCEKCOeTw</th>\n",
       "      <td>17.31</td>\n",
       "      <td>-44.51</td>\n",
       "      <td>21.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>bJKCEKCOeTw</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>267.00</td>\n",
       "      <td>...</td>\n",
       "      <td>191.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PBB4SHQHTbY</th>\n",
       "      <td>12.59</td>\n",
       "      <td>44.83</td>\n",
       "      <td>11.97</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>PBB4SHQHTbY</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>260.00</td>\n",
       "      <td>...</td>\n",
       "      <td>61.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9zb0Oo0ryEI</th>\n",
       "      <td>14.32</td>\n",
       "      <td>28.18</td>\n",
       "      <td>13.64</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>9zb0Oo0ryEI</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>398.00</td>\n",
       "      <td>...</td>\n",
       "      <td>61.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>91.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.13</td>\n",
       "      <td>3.00</td>\n",
       "      <td>84.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97uiV4RiSAY</th>\n",
       "      <td>19.30</td>\n",
       "      <td>13.90</td>\n",
       "      <td>16.04</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>97uiV4RiSAY</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>477.00</td>\n",
       "      <td>...</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1,346.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>1.27</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.83</td>\n",
       "      <td>66.00</td>\n",
       "      <td>1,142.00</td>\n",
       "      <td>19.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m-PozFAV1xE</th>\n",
       "      <td>21.88</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>19.47</td>\n",
       "      <td>12.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>m-PozFAV1xE</td>\n",
       "      <td>7.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>870.00</td>\n",
       "      <td>...</td>\n",
       "      <td>39.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>63.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>59.00</td>\n",
       "      <td>2.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LedWl2QTy40</th>\n",
       "      <td>18.83</td>\n",
       "      <td>32.11</td>\n",
       "      <td>16.94</td>\n",
       "      <td>99.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>LedWl2QTy40</td>\n",
       "      <td>20.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>6.00</td>\n",
       "      <td>119.00</td>\n",
       "      <td>...</td>\n",
       "      <td>715.00</td>\n",
       "      <td>0.02</td>\n",
       "      <td>15.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.22</td>\n",
       "      <td>2.00</td>\n",
       "      <td>14.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GNghXeGlTdI</th>\n",
       "      <td>11.78</td>\n",
       "      <td>61.92</td>\n",
       "      <td>10.33</td>\n",
       "      <td>85.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>GNghXeGlTdI</td>\n",
       "      <td>23.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.00</td>\n",
       "      <td>684.00</td>\n",
       "      <td>...</td>\n",
       "      <td>322.00</td>\n",
       "      <td>0.04</td>\n",
       "      <td>20.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1.00</td>\n",
       "      <td>18.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d360OEFeVAQ</th>\n",
       "      <td>11.40</td>\n",
       "      <td>59.03</td>\n",
       "      <td>10.16</td>\n",
       "      <td>107.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>d360OEFeVAQ</td>\n",
       "      <td>32.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.00</td>\n",
       "      <td>167.00</td>\n",
       "      <td>...</td>\n",
       "      <td>494.00</td>\n",
       "      <td>0.01</td>\n",
       "      <td>21.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>18.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>QTcNp2JRgoU</th>\n",
       "      <td>18.46</td>\n",
       "      <td>-53.05</td>\n",
       "      <td>22.15</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>QTcNp2JRgoU</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>401.00</td>\n",
       "      <td>...</td>\n",
       "      <td>39.00</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PBB4SHQHTbY</th>\n",
       "      <td>12.59</td>\n",
       "      <td>44.83</td>\n",
       "      <td>11.97</td>\n",
       "      <td>4.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>PBB4SHQHTbY</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>260.00</td>\n",
       "      <td>...</td>\n",
       "      <td>62.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>24.00</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11143 rows × 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             ARI     FleshReadingEase              Kincaid  \\\n",
       "video_id                                                                     \n",
       "bJKCEKCOeTw                17.31               -44.51                21.45   \n",
       "PBB4SHQHTbY                12.59                44.83                11.97   \n",
       "9zb0Oo0ryEI                14.32                28.18                13.64   \n",
       "97uiV4RiSAY                19.30                13.90                16.04   \n",
       "m-PozFAV1xE                21.88                -0.24                19.47   \n",
       "...                          ...                  ...                  ...   \n",
       "LedWl2QTy40                18.83                32.11                16.94   \n",
       "GNghXeGlTdI                11.78                61.92                10.33   \n",
       "d360OEFeVAQ                11.40                59.03                10.16   \n",
       "QTcNp2JRgoU                18.46               -53.05                22.15   \n",
       "PBB4SHQHTbY                12.59                44.83                11.97   \n",
       "\n",
       "                     active_verb      has_description           id  \\\n",
       "video_id                                                             \n",
       "bJKCEKCOeTw                 0.00                 1.00  bJKCEKCOeTw   \n",
       "PBB4SHQHTbY                 4.00                 1.00  PBB4SHQHTbY   \n",
       "9zb0Oo0ryEI                 4.00                 1.00  9zb0Oo0ryEI   \n",
       "97uiV4RiSAY                 1.00                 1.00  97uiV4RiSAY   \n",
       "m-PozFAV1xE                12.00                 1.00  m-PozFAV1xE   \n",
       "...                          ...                  ...          ...   \n",
       "LedWl2QTy40                99.00                 1.00  LedWl2QTy40   \n",
       "GNghXeGlTdI                85.00                 1.00  GNghXeGlTdI   \n",
       "d360OEFeVAQ               107.00                 1.00  d360OEFeVAQ   \n",
       "QTcNp2JRgoU                 1.00                 1.00  QTcNp2JRgoU   \n",
       "PBB4SHQHTbY                 4.00                 1.00  PBB4SHQHTbY   \n",
       "\n",
       "                  sentence_count        summary_words     transition_words  \\\n",
       "video_id                                                                     \n",
       "bJKCEKCOeTw                 1.00                 0.00                 1.00   \n",
       "PBB4SHQHTbY                 2.00                 0.00                 2.00   \n",
       "9zb0Oo0ryEI                 3.00                 0.00                 2.00   \n",
       "97uiV4RiSAY                 1.00                 0.00                 1.00   \n",
       "m-PozFAV1xE                 7.00                 0.00                 2.00   \n",
       "...                          ...                  ...                  ...   \n",
       "LedWl2QTy40                20.00                 0.00                 6.00   \n",
       "GNghXeGlTdI                23.00                 0.00                 8.00   \n",
       "d360OEFeVAQ                32.00                 0.00                 7.00   \n",
       "QTcNp2JRgoU                 1.00                 0.00                 1.00   \n",
       "PBB4SHQHTbY                 2.00                 0.00                 2.00   \n",
       "\n",
       "                  video_duration  ...    channelVideoCount  \\\n",
       "video_id                          ...                        \n",
       "bJKCEKCOeTw               267.00  ...               191.00   \n",
       "PBB4SHQHTbY               260.00  ...                61.00   \n",
       "9zb0Oo0ryEI               398.00  ...                61.00   \n",
       "97uiV4RiSAY               477.00  ...                 3.00   \n",
       "m-PozFAV1xE               870.00  ...                39.00   \n",
       "...                          ...  ...                  ...   \n",
       "LedWl2QTy40               119.00  ...               715.00   \n",
       "GNghXeGlTdI               684.00  ...               322.00   \n",
       "d360OEFeVAQ               167.00  ...               494.00   \n",
       "QTcNp2JRgoU               401.00  ...                39.00   \n",
       "PBB4SHQHTbY               260.00  ...                62.00   \n",
       "\n",
       "             keyword_decription_cosine  comment_total_words  \\\n",
       "video_id                                                      \n",
       "bJKCEKCOeTw                       0.09                 0.00   \n",
       "PBB4SHQHTbY                       0.00                 5.00   \n",
       "9zb0Oo0ryEI                       0.00                91.00   \n",
       "97uiV4RiSAY                       0.00             1,346.00   \n",
       "m-PozFAV1xE                       0.03                63.00   \n",
       "...                                ...                  ...   \n",
       "LedWl2QTy40                       0.02                15.00   \n",
       "GNghXeGlTdI                       0.04                20.00   \n",
       "d360OEFeVAQ                       0.01                21.00   \n",
       "QTcNp2JRgoU                       0.35                 0.00   \n",
       "PBB4SHQHTbY                       0.00                24.00   \n",
       "\n",
       "            negative_comment_count  comment_description_cosine  \\\n",
       "video_id                                                         \n",
       "bJKCEKCOeTw                   0.00                        0.00   \n",
       "PBB4SHQHTbY                   0.00                        0.00   \n",
       "9zb0Oo0ryEI                   2.00                        0.49   \n",
       "97uiV4RiSAY                  14.00                        1.27   \n",
       "m-PozFAV1xE                   0.00                        0.03   \n",
       "...                            ...                         ...   \n",
       "LedWl2QTy40                   0.00                        0.09   \n",
       "GNghXeGlTdI                   0.00                        0.28   \n",
       "d360OEFeVAQ                   0.00                        0.22   \n",
       "QTcNp2JRgoU                   0.00                        0.00   \n",
       "PBB4SHQHTbY                   1.00                        0.07   \n",
       "\n",
       "            keyword_title_cosine comment_title_cosine postive_comment_count  \\\n",
       "video_id                                                                      \n",
       "bJKCEKCOeTw                 0.09                 0.00                  0.00   \n",
       "PBB4SHQHTbY                 0.00                 0.00                  0.00   \n",
       "9zb0Oo0ryEI                 0.17                 0.13                  3.00   \n",
       "97uiV4RiSAY                 0.20                 0.83                 66.00   \n",
       "m-PozFAV1xE                 0.00                 0.00                  7.00   \n",
       "...                          ...                  ...                   ...   \n",
       "LedWl2QTy40                 0.00                 0.22                  2.00   \n",
       "GNghXeGlTdI                 0.00                 0.27                  1.00   \n",
       "d360OEFeVAQ                 0.00                 0.00                  1.00   \n",
       "QTcNp2JRgoU                 0.00                 0.00                  0.00   \n",
       "PBB4SHQHTbY                 0.00                 0.00                  1.00   \n",
       "\n",
       "            comment_unique_words  neutral_comment_count  \n",
       "video_id                                                 \n",
       "bJKCEKCOeTw                 0.00                   0.00  \n",
       "PBB4SHQHTbY                 5.00                   4.00  \n",
       "9zb0Oo0ryEI                84.00                   1.00  \n",
       "97uiV4RiSAY             1,142.00                  19.00  \n",
       "m-PozFAV1xE                59.00                   2.00  \n",
       "...                          ...                    ...  \n",
       "LedWl2QTy40                14.00                   0.00  \n",
       "GNghXeGlTdI                18.00                   0.00  \n",
       "d360OEFeVAQ                18.00                   1.00  \n",
       "QTcNp2JRgoU                 0.00                   0.00  \n",
       "PBB4SHQHTbY                24.00                   5.00  \n",
       "\n",
       "[11143 rows x 55 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a6b4e2",
   "metadata": {},
   "source": [
    "# Part 1.3: Merging videos with PEMAT labels with overall 12k.\n",
    "In this part, I merge df (12k) with df_label (600) based on URL. Afterwards, we will obtain a dataset with 600 labelled videos along with all of its information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fd783f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the matching based on 'URL' in df_label and 'id' in df.\n",
    "\n",
    "tempdf = df_label[[\"URL\",\"Duration\",\"info\", \"action\", \"understand\"]]\n",
    "\n",
    "# Find all videos in the large dataset with labels. \n",
    "# Results: The dataset has some duplicate. \n",
    "tempdf = tempdf.set_index(\"URL\")\n",
    "try:\n",
    "    df = df.set_index(\"video_id\")\n",
    "except:\n",
    "    pass\n",
    "newdf = tempdf.join(df)\n",
    "\n",
    "# Dropping duplicate entries and storing the cleaned dataset\n",
    "try:\n",
    "    newdf.reset_index(inplace = True)\n",
    "except:\n",
    "    pass \n",
    "newdf = newdf.rename(columns = {\"index\":\"URL\"})\n",
    "newdf = newdf.drop_duplicates(\"URL\")\n",
    "df600 = newdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "eb30ad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert every features whose values can be interpreted as numbers.\n",
    "for col in df600.columns.tolist():\n",
    "    df600[col] = pd.to_numeric(df600[col], errors = \"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8909bc4",
   "metadata": {},
   "source": [
    "# Part 1.4: Use Xiao's readability.\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339d2b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataframe on readability for each video. Rename the dataframe\n",
    "readability_df = readability_df.set_index(\"video_id\")\n",
    "df = df.set_index(\"URL\")\n",
    "# Re-assign readability indices\n",
    "for index in df.index.tolist():\n",
    "    if index in readability_df.index.tolist():\n",
    "        df.loc[index, \"ARI\"] = readability_df.loc[index, \"ari\"] \n",
    "        df.loc[index, \"FleshReadingEase\"] = readability_df.loc[index, \"flesch\"]\n",
    "        df.loc[index, \"Kincaid\"] = readability_df.loc[index, \"kincaid\"] \n",
    "# Save all files\n",
    "df_label.to_csv(\"rawPEMAT.csv\")\n",
    "df.to_csv(\"merged_and_cleaned600.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f07395",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"categoryId\"] = [str(category) for category in df[\"categoryId\"]]\n",
    "df.to_csv(\"merged_and_cleaned600.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250bfc50",
   "metadata": {},
   "source": [
    "# Part 2: Checking PEMAT input per questions\n",
    "\n",
    "In the above part, I have merged Xiao's datasets and created two versions: merged_and_cleaned with labels (600) and merged_and_cleaned without labels (11000). The remaining task would be to join them by row_id, if need be. However, it's interesting to examine df_label itself to see what the distribution of each questions are.\n",
    "\n",
    "## Missing values\n",
    "For most of the columns, there are only few missing entries. The noteworthy columns with high misses are 13, 18, 19, 25, which corresponds to clarity of simple graphs/illustrations/etc. These questions may not be applicable to all diabetes videos. Therefore, there's nothing egregious about missingness. \n",
    "\n",
    "## Strange values\n",
    "Medical information (i.e. 'info') has a lot of non-sense values. \n",
    "\n",
    "## Observations\n",
    "1. 75% of the videos are understandable (1-19), 25% are not. A simple rule of outputting 1 (i.e. every video is understandable) will achieve 75% accuracy, so **need to think about alternative metrics**. 47% of videos are actionable (corresponding to Question 20,21,22,25).\n",
    "\n",
    "2. Because some PEMAT questions have NaN, I cannot calculate the correlation between each of the response questions.\n",
    "TODO: Ask Larry/Nynke if there's a need to calculate the correlation.\n",
    "\n",
    "3. Actionability and understandability have the correlation coefficient of 0.128. This value is very low. \n",
    "\n",
    "4. Duration (unit: second) is skewed heavily to the right — there are some very lengthy videos. **Need to log-scale if included in the final model.**\n",
    "\n",
    "5. PEMAT criteria with mostly zero entries are 8,9,19,22 (check PEMAT_dict). They all correspond to not breaking down information into small-chunks/actionable steps or lack of informative headers. **This could inform why videos we classify as zero are not understandable/actionable**. \n",
    "\n",
    "6. \"11\" (summary) and \"25\" (graphs/charts to take actions) do not receive good scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931e88e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking missing values of PEMAT response in each column. \n",
    "df_label.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341dce33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution\n",
    "df_label[\"Duration\"] = pd.to_numeric(df_label[\"Duration\"],)\n",
    "numerical_col = df_label.columns.tolist()\n",
    "numerical_col.remove(\"Title\")\n",
    "numerical_col.remove(\"URL\")\n",
    "fare = [\"min\", \"max\", \"median\", \"mean\"]\n",
    "temp = {}\n",
    "for col in numerical_col:\n",
    "    temp[col] = fare\n",
    "df_label.agg(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2819d349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A code to find the covaraince matrix for each of the PEMAT questions. \n",
    "temp = []\n",
    "for col in  df_label.columns.tolist()[3:19]:\n",
    "    temp.append(df_label[col].tolist())\n",
    "temp = np.array(temp)\n",
    "\n",
    "# TODO: uncomment. np.corrcoef(temp)\n",
    "\n",
    "# Find the correlation between understand and action\n",
    "act_under = np.array([df_label[\"action\"].tolist(), df_label[\"understand\"]])\n",
    "np.corrcoef(act_under)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633cb3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the location where info label is unavailable.\n",
    "np.where(df_label[\"info\"].isna() == True)\n",
    "df = df.drop(index = [275])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
